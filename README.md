# üéØ Scraper Prospects B2B - Documentation Compl√®te

Outil automatis√© pour scraper, enrichir et qualifier des prospects PME fran√ßaises via :
- **Data.gouv.fr** (base publique des entreprises)
- **Pappers** (enrichissement donn√©es)
- **Claude AI** (qualification intelligente)

---

## üìã Table des mati√®res

1. [Installation](#installation)
2. [Configuration](#configuration)
3. [Utilisation - Interface Web](#utilisation-interface-web)
4. [Utilisation - Ligne de commande](#utilisation-ligne-de-commande)
5. [Automatisation avec n8n/Make](#automatisation-avec-n8nmake)
6. [Structure des fichiers](#structure-des-fichiers)
7. [FAQ](#faq)

---

## üöÄ Installation

### Pr√©requis

- **Python 3.9+** install√©
- Compte **Anthropic** (pour la qualification IA)

### √âtapes

1. **Clone ou t√©l√©charge le dossier** `prospect-scraper/`

2. **Installe les d√©pendances** :

```bash
cd prospect-scraper
pip install -r requirements.txt
```

3. **Cr√©e le dossier de sortie** :

```bash
mkdir outputs
```

---

## ‚öôÔ∏è Configuration

### 1. Cl√© API Anthropic

Obtiens ta cl√© API sur [console.anthropic.com](https://console.anthropic.com/)

√âdite **`config.py`** et remplace :

```python
ANTHROPIC_API_KEY = "sk-ant-xxxxx"  # ‚Üê Mets ta vraie cl√© ici
```

### 2. Personnalise les filtres (optionnel)

Dans **`config.py`**, modifie les filtres par d√©faut :

```python
FILTRES = {
    "ca_min": 5_000_000,      # 5 M‚Ç¨
    "ca_max": 50_000_000,     # 50 M‚Ç¨
    "region": "11",           # √éle-de-France
    "secteur_naf": "62",      # Programmation informatique
    "forme_juridique": "SAS",
    "limit": 50,              # Pour tests
}
```

**Codes utiles** :

R√©gions :
- `11` = √éle-de-France
- `84` = Auvergne-Rh√¥ne-Alpes
- `93` = PACA
- `None` = Toute la France

Secteurs NAF :
- `62` = Informatique
- `41-43` = Construction
- `46-47` = Commerce
- `69-74` = Services B2B
- `None` = Tous secteurs

---

## üñ•Ô∏è Utilisation - Interface Web

### Lancement

```bash
streamlit run app.py
```

L'interface s'ouvre dans ton navigateur.

### Workflow

1. **Configure les filtres** :
   - CA min/max
   - R√©gion
   - Secteur NAF
   - Forme juridique
   - Limite de r√©sultats (pour tester)

2. **Entre ta cl√© API** Anthropic dans la sidebar

3. **Lance la recherche** ‚Üí Le syst√®me :
   - ‚úÖ Scrape data.gouv.fr
   - ‚úÖ Enrichit via Pappers
   - ‚úÖ Qualifie avec Claude AI
   - ‚úÖ G√©n√®re un Excel avec tout dedans

4. **T√©l√©charge le fichier** Excel final

### Exemple de r√©sultat

Fichier Excel avec :
- Score A/B/C/D
- Nom, SIREN, CA, secteur
- Dirigeant, t√©l√©phone, email, site web
- R√©sum√© business (3-4 lignes)
- Analyse corporate fit
- Justification du score

---

## üíª Utilisation - Ligne de commande

Si tu pr√©f√®res les scripts :

### 1. Scraping data.gouv

```bash
python scraper.py
```

G√©n√®re : `outputs/data_gouv_raw_YYYYMMDD_HHMMSS.xlsx`

### 2. Enrichissement Pappers

```bash
python enricher.py
```

Prend le fichier brut le plus r√©cent, l'enrichit.

G√©n√®re : `outputs/enriched_YYYYMMDD_HHMMSS.xlsx`

### 3. Qualification IA

```bash
python qualifier.py
```

Prend le fichier enrichi, qualifie chaque prospect.

G√©n√®re : `outputs/prospects_qualified_YYYYMMDD_HHMMSS.xlsx`

### Pipeline complet (1 commande)

Cr√©e un script `run_all.py` :

```python
import os
from datetime import datetime
from scraper import DataGouvScraper
from enricher import PappersEnricher
from qualifier import ProspectQualifier
import config

# Filtres
filtres = config.FILTRES

# 1. Scraping
scraper = DataGouvScraper()
companies = scraper.search_companies(filtres)
df = scraper.to_dataframe(companies)

# 2. Enrichissement
enricher = PappersEnricher()
df = enricher.enrich_dataframe(df)

# 3. Qualification
qualifier = ProspectQualifier(config.ANTHROPIC_API_KEY)
df = qualifier.qualify_dataframe(df)

# 4. Export
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output = f"outputs/prospects_qualified_{timestamp}.xlsx"
qualifier.format_excel_output(df, output)

print(f"\n‚úÖ Termin√© : {output}")
```

Puis lance :

```bash
python run_all.py
```

---

## ü§ñ Automatisation avec n8n/Make

### Option 1 : n8n (Recommand√© pour self-hosted)

**Architecture** :

```
Trigger (Cron 1x/semaine)
    ‚Üì
HTTP Request ‚Üí Python script sur serveur
    ‚Üì
Webhook ‚Üí R√©cup√®re Excel
    ‚Üì
Google Drive ‚Üí Upload du fichier
    ‚Üì
Slack/Email ‚Üí Notification
```

**√âtapes** :

1. **H√©berge le script Python** sur un serveur (VPS, Render, Heroku)

2. **Cr√©e une API Flask** pour exposer le scraper :

```python
# api.py
from flask import Flask, send_file, request
import run_all  # Ton script

app = Flask(__name__)

@app.route('/scrape', methods=['POST'])
def scrape():
    # R√©cup√®re les filtres du POST
    filtres = request.json
    
    # Lance le pipeline
    output_file = run_all.run_pipeline(filtres)
    
    # Retourne le fichier
    return send_file(output_file, as_attachment=True)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

3. **Dans n8n** :

- Node **Schedule Trigger** : Cron `0 9 * * 1` (tous les lundis 9h)
- Node **HTTP Request** : POST vers ton API `/scrape`
- Node **Google Drive** : Upload du fichier
- Node **Slack** : Notification "Nouveau fichier prospects dispo !"

### Option 2 : Make (Zapier-like, no-code)

**Workflow** :

1. **Trigger** : Schedule (1x/semaine)
2. **HTTP** : Appelle ton API Python
3. **Webhooks** : Re√ßoit le fichier Excel
4. **Google Drive** : Upload
5. **Email** : Notification

**Limitations** : Make a des limites de timeout (quelques minutes max), donc il faut que ton scraper soit rapide ou d√©coupe en plusieurs √©tapes.

### Option 3 : GitHub Actions (Gratuit)

Cr√©e `.github/workflows/scrape.yml` :

```yaml
name: Scrape Prospects

on:
  schedule:
    - cron: '0 9 * * 1'  # Tous les lundis 9h
  workflow_dispatch:  # Bouton manuel

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: pip install -r requirements.txt
      
      - name: Run scraper
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: python run_all.py
      
      - name: Upload to Google Drive
        uses: satackey/action-google-drive@v1
        with:
          credentials: ${{ secrets.GOOGLE_DRIVE_CREDENTIALS }}
          file: outputs/prospects_qualified_*.xlsx
```

---

## üìÅ Structure des fichiers

```
prospect-scraper/
‚îÇ
‚îú‚îÄ‚îÄ config.py              # Configuration et filtres
‚îú‚îÄ‚îÄ scraper.py             # Scraper data.gouv.fr
‚îú‚îÄ‚îÄ enricher.py            # Enrichissement Pappers
‚îú‚îÄ‚îÄ qualifier.py           # Qualification IA Claude
‚îú‚îÄ‚îÄ app.py                 # Interface Streamlit
‚îú‚îÄ‚îÄ requirements.txt       # D√©pendances Python
‚îú‚îÄ‚îÄ README.md              # Cette doc
‚îÇ
‚îî‚îÄ‚îÄ outputs/               # Fichiers g√©n√©r√©s
    ‚îú‚îÄ‚îÄ raw_*.xlsx         # Donn√©es brutes data.gouv
    ‚îú‚îÄ‚îÄ enriched_*.xlsx    # Donn√©es enrichies Pappers
    ‚îî‚îÄ‚îÄ prospects_qualified_*.xlsx  # Fichier final avec scoring
```

---

## ‚ùì FAQ

### Combien de temps √ßa prend ?

- **50 entreprises** : ~5-10 min
- **200 entreprises** : ~30-40 min
- **500 entreprises** : ~1h30-2h

Les d√©lais viennent surtout de :
- Pappers (2 sec par entreprise pour √©viter le ban)
- API Claude (rate limiting)

### Combien √ßa co√ªte ?

**Data.gouv** : Gratuit ‚úÖ

**Pappers** : 
- Scraping l√©ger = gratuit
- API payante = 49‚Ç¨/mois (optionnel)

**Claude API** :
- ~$0.015 par analyse
- 100 prospects = ~$1.50
- 1000 prospects = ~$15

**Total** : Quasiment gratuit pour <500 prospects/mois.

### Puis-je utiliser sans cl√© API ?

Oui, mais tu n'auras pas la qualification IA.

Tu peux :
- Scraper data.gouv
- Enrichir via Pappers
- Exporter l'Excel brut

Ensuite, qualifie manuellement ou via une autre IA.

### Les donn√©es sont-elles l√©gales ?

**100% l√©gales** ‚úÖ

- Data.gouv = donn√©es publiques (Open Data)
- Pappers = registre public des entreprises
- Pas de donn√©es personnelles sensibles
- Usage B2B professionnel autoris√©

### Puis-je changer les crit√®res de scoring ?

Oui ! √âdite le prompt dans `qualifier.py`, fonction `build_analysis_prompt()`.

Exemple : ajouter un crit√®re "pr√©sence digitale" :

```python
prompt = f"""
...
6. Pr√©sence digitale : site web moderne/ancien, SEO, r√©seaux sociaux
...
"""
```

### Puis-je ajouter d'autres sources ?

Oui ! Cr√©e un nouveau module `enricher_X.py` et int√®gre-le dans le pipeline.

Exemples de sources :
- Societe.com
- Infogreffe
- LinkedIn (via API Sales Navigator)
- Google Places

---

## üÜò Support

**Probl√®mes courants** :

1. **"No module named X"** ‚Üí `pip install -r requirements.txt`
2. **"API key invalid"** ‚Üí V√©rifie ta cl√© Anthropic dans `config.py`
3. **Rate limit exceeded** ‚Üí Augmente `delay_between_requests` dans config
4. **Pas de r√©sultats** ‚Üí √âlargis les filtres (CA, r√©gion, secteur)

---

## üìä Exemple de r√©sultat final

| Score | Nom | CA (M‚Ç¨) | Ville | Justification |
|-------|-----|---------|-------|---------------|
| **A** | TechCorp SAS | 12.5 | Paris | Fondateur majoritaire, CA croissant, aucun LBO visible |
| **A** | IndusPro SARL | 18.3 | Lyon | PME familiale, potentiel transmission, secteur porteur |
| **B** | ServicePlus | 8.2 | Marseille | CA stable, dirigeant en place depuis 15 ans |
| **C** | MiniCo | 6.1 | Lille | Petite structure, secteur tr√®s concurrentiel |

---

## üöÄ Prochaines √©tapes

1. **Teste avec 10-20 entreprises** pour valider
2. **Affine les filtres** selon tes cibles r√©elles
3. **Automatise** avec n8n ou GitHub Actions
4. **Ajoute des sources** (LinkedIn, Societe.com)
5. **Int√®gre ton CRM** (Pipedrive, HubSpot, Salesforce)

---

**Bonne prospection ! üéØ**
